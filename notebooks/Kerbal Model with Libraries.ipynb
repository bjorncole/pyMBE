{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mediterranean-charleston",
   "metadata": {},
   "source": [
    "# Part Combination Space Exploration\n",
    "\n",
    "This is a notebook developed to leverage the new SysML v2 semantics for nested features and instantiation of models to generate instances of M1 system models as feedstock for analysis pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import math\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import sysml_v2_api_client\n",
    "from sysml_v2_api_client.rest import ApiException\n",
    "from pprint import pprint\n",
    "\n",
    "import json\n",
    "import networkx as NX\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from pymbe.model_loading import ModelingSession as Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-matter",
   "metadata": {},
   "source": [
    "# Configure API Server Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysml_api_base_url = 'http://sysml2-sst.intercax.com:9000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-theorem",
   "metadata": {},
   "source": [
    "## Activate APIs\n",
    "\n",
    "Connect the API classes to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = sysml_v2_api_client.Configuration(\n",
    "    host = sysml_api_base_url\n",
    ")\n",
    "\n",
    "projects_api_instance = None\n",
    "\n",
    "with sysml_v2_api_client.ApiClient(configuration) as api_client:\n",
    "    # Create an instance of the API class\n",
    "    project_api_instance = sysml_v2_api_client.ProjectApi(api_client)\n",
    "    \n",
    "commits_api_instance = None\n",
    "\n",
    "with sysml_v2_api_client.ApiClient(configuration) as api_client:\n",
    "    # Create an instance of the API class\n",
    "    commits_api_instance = sysml_v2_api_client.CommitApi(api_client)\n",
    "    \n",
    "elements_api_instance = None\n",
    "\n",
    "with sysml_v2_api_client.ApiClient(configuration) as api_client:\n",
    "    # Create an instance of the API class\n",
    "    elements_api_instance = sysml_v2_api_client.ElementApi(api_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-tomorrow",
   "metadata": {},
   "source": [
    "## Pull down commits and elements catalogs\n",
    "\n",
    "With the API handles, use the pre-built methods to get lists of commits and elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_api_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "kerbal_proj = [my_proj for my_proj in project_api_instance.get_projects() if my_proj.name.find('Kerbal') > -1][0]\n",
    "kerbal_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get commits by project\n",
    "    commits_response = commits_api_instance.get_commits_by_project(kerbal_proj.id)\n",
    "    pprint(commits_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling CommitApi->get_commits_by_project: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = []\n",
    "\n",
    "try:\n",
    "    # Get commits by project\n",
    "    elements = elements_api_instance.get_elements_by_project_commit(kerbal_proj.id, commits_response[0].id)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling ElementApi->get_elements_by_project_commit: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(elements[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-wales",
   "metadata": {},
   "source": [
    "# Gather Element Data\n",
    "\n",
    "Since the generated API doesn't have much detail for elements, need to do this more hands-on.\n",
    "\n",
    "Not elegant below to just have a larger page size, should implement paging later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_url = (sysml_api_base_url +\n",
    "                '/projects/{0}/commits/{1}/elements?page[size]=2000').format(kerbal_proj.id, commits_response[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_response = requests.get(\n",
    "    elements_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_data = elements_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-sheet",
   "metadata": {},
   "source": [
    "Split the elements into relationships and non-relationships. This will let us work with graph representations and a graph understanding of the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elements_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_relations = [non_relation for non_relation in elements_data if not 'relatedElement' in non_relation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [relation for relation in elements_data if 'relatedElement' in relation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-trademark",
   "metadata": {},
   "source": [
    "Survey which and how many metatypes are in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "metatypes = []\n",
    "for nr in elements_data:\n",
    "    if nr['@type'] not in metatypes:\n",
    "        metatypes.append(nr['@type'])\n",
    "        \n",
    "metatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-galaxy",
   "metadata": {},
   "source": [
    "Create a working session for the model and feed it the serialized data. The working session will also generate useful graphs to inspect later in this workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_model = Session()\n",
    "working_model.thaw_json_data(elements_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-islam",
   "metadata": {},
   "source": [
    "## Show Computed Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-newton",
   "metadata": {},
   "source": [
    "### Superclassing Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_labels = NX.get_node_attributes(working_model.graph_manager.superclassing_graph,'name')\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "NX.draw_planar(working_model.graph_manager.superclassing_graph,\n",
    "               labels=super_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-external",
   "metadata": {},
   "source": [
    "### Banded Attribute Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(working_model.graph_manager.banded_featuring_graph.successors('f5f406e8-b8a6-4f8b-a90e-01616a6cf1c1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "banded_labels = NX.get_node_attributes(working_model.graph_manager.banded_featuring_graph,'name')\n",
    "\n",
    "edge_kinds = NX.get_edge_attributes(working_model.graph_manager.banded_featuring_graph,'kind')\n",
    "colors = {}\n",
    "\n",
    "for key, value in edge_kinds.items():\n",
    "    if value == 'Superclassing':\n",
    "        colors.update({key: 'b'})\n",
    "    elif value == 'FeatureTyping':\n",
    "        colors.update({key: 'g'})\n",
    "    elif value == 'FeatureMembership':\n",
    "        colors.update({key: 'r'})\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "NX.draw_planar(working_model.graph_manager.banded_featuring_graph,\n",
    "               labels=banded_labels,\n",
    "              edge_color=colors.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-breed",
   "metadata": {},
   "source": [
    "## Inspect Part Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_uses = working_model.get_all_of_metaclass(metaclass_name='PartUsage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(part_uses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "['{0} has multiplicity {1}..{2}'.format(\n",
    "    part_use['name'],\n",
    "    working_model.feature_lower_multiplicity(part_use['@id']),\n",
    "    working_model.feature_upper_multiplicity(part_use['@id'])\n",
    ") for part_use in part_uses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-grocery",
   "metadata": {},
   "source": [
    "Get feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = [feature_type for feature_type in relations if feature_type['@type'] == 'FeatureTyping']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-ballot",
   "metadata": {},
   "source": [
    "# Refactor still in progress below this line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-forwarding",
   "metadata": {},
   "source": [
    "# Generate Instances\n",
    "\n",
    "With the base semantic model in hand, begin to apply the rules to generate our system alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-president",
   "metadata": {},
   "source": [
    "## Find number of instances for feature last positions\n",
    "\n",
    "In SysML, the default type is PartDefinition, which is a Classifier, meaning the minimal interpretation of length one (the specific instance). Nesting parts then have an interpretation as expected by systems engineers, namely that the instances \"stack\" in order to provide a navigation from top-level assembly to leaf component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "['{0} needs {1} instances of type {2}'.format(\n",
    "    part_use['name'],\n",
    "    working_model.feature_upper_multiplicity(part_use['@id']),\n",
    "    working_model.graph_manager.get_feature_type_name(part_use['@id'])\n",
    ") for part_use in part_uses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-reality",
   "metadata": {},
   "source": [
    "Automatically shorten names so that sequences remain readable when printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten_pre_bake = {\n",
    "    'RT-10 \"Hammer\" Solid Fuel Booster': \"RT-10\",\n",
    "    'RT-5 \"Flea\" Solid Fuel Booster': \"RT-5\",\n",
    "    'LV-T45 \"Swivel\" Liquid Fuel Engine': \"LV-T45\",\n",
    "    'FL-T100 Fuel Tank': \"FL-T100\",\n",
    "    'FL-T200 Fuel Tank': \"FL-T200\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_name(name, shorten_pre_bake=None):\n",
    "    short_name = ''\n",
    "    if len(name) > 5:\n",
    "        if shorten_pre_bake is not None:\n",
    "            if name in shorten_pre_bake:\n",
    "                return shorten_pre_bake[name]\n",
    "        space_place = name.find(' ')\n",
    "        if space_place > -1:\n",
    "            short_name = short_name + name[0]\n",
    "            short_name = short_name + name[space_place + 1]\n",
    "            next_space = name.find(' ', space_place + 1)\n",
    "            while next_space > -1:\n",
    "                short_name = short_name + name[next_space + 1]\n",
    "                next_space = name.find(' ', next_space + 1)\n",
    "            return short_name\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance():\n",
    "    def __init__(self, name, index):\n",
    "        self.name = shorten_name(name) + '#' + str(index)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicity_dict = {}\n",
    "\n",
    "for part_use in part_uses:\n",
    "    type_name = get_feature_type_name(part_use)\n",
    "    if type_name in multiplicity_dict:\n",
    "        old_val = multiplicity_dict[part_use]\n",
    "        multiplicity_dict.update({type_name: old_val + feature_upper_multiplicity(part_use)})\n",
    "    else:\n",
    "        multiplicity_dict.update({type_name: feature_upper_multiplicity(part_use)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-writer",
   "metadata": {},
   "source": [
    "## Determine the size of the universe of instances needed for creating alternatives\n",
    "\n",
    "Use feature membership together with multiplicity to decide how many individuals are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_leaves = [model_loading.lookup.id_memo_dict[node]\n",
    "    for node in working_model.graph_manager.part_featuring_graph.nodes\n",
    "        if working_model.graph_manager.part_featuring_graph.in_degree(node) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_roots = [model_loading.lookup.id_memo_dict[node]\n",
    "    for node in working_model.graph_manager.part_featuring_graph.nodes\n",
    "        if working_model.graph_manager.part_featuring_graph.out_degree(node) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "banded_roots = [model_loading.lookup.id_memo_dict[node]\n",
    "    for node in working_model.graph_manager.banded_featuring_graph.nodes\n",
    "        if working_model.graph_manager.banded_featuring_graph.out_degree(node) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "[part_leaf['name'] for part_leaf in part_leaves]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "[part_root['name'] for part_root in part_roots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "[banded_root['name'] for banded_root in banded_roots]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-mining",
   "metadata": {},
   "source": [
    "Correct the multiplicities by considering nesting.part_uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_multiplicity = {}\n",
    "\n",
    "for part_use in part_uses:\n",
    "    corrected_mult = 0\n",
    "    for part_tree_root in banded_roots:\n",
    "        try:\n",
    "            part_path = NX.shortest_path(banded_feature_graph, part_use['@id'], part_tree_root['@id'])\n",
    "            # TODO: check that the path actually exists\n",
    "            corrected_mult = math.prod([feature_upper_multiplicity(id_memo_dict[node]) for node in part_path])\n",
    "        except NX.NetworkXNoPath:\n",
    "            pass\n",
    "    part_multiplicity.update({part_use['@id']: corrected_mult})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_multiplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-therapy",
   "metadata": {},
   "source": [
    "### Subdivide Abstract Feature Types\n",
    "\n",
    "Look at the feature types for where they are abstract and then generate instances from the more specific types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "[get_name_by_id(node) for node in subclassing_graph.nodes if subclassing_graph.out_degree(node) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_multiplicity_dict = {}\n",
    "type_id_pairs = {}\n",
    "\n",
    "for part_use in part_uses:\n",
    "    type_name = get_feature_type_name(part_use)\n",
    "    if type_name in corrected_multiplicity_dict:\n",
    "        old_val = corrected_multiplicity_dict[part_use]\n",
    "        corrected_multiplicity_dict.update({type_name: old_val + part_multiplicity[part_use['@id']]})\n",
    "    else:\n",
    "        corrected_multiplicity_dict.update({type_name: part_multiplicity[part_use['@id']]})\n",
    "        if len(part_use['type']) > 0:\n",
    "            type_id_pairs.update({type_name: part_use['type'][0]['@id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_id_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_multiplicity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-pride",
   "metadata": {},
   "source": [
    "## Generate Random Alternatives\n",
    "\n",
    "Start creating the alternatives with random draws on multiplicity. This will be our space for investigation for weights, thrust-to-weight ratios at stage ignitions, delta-Vs, and initial and burnout masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_alts_to_create = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_multiplicity_dicts = []\n",
    "\n",
    "for step in range(0, no_alts_to_create):\n",
    "\n",
    "    partitioned_multiplicity_dict = {}\n",
    "\n",
    "    for key in corrected_multiplicity_dict:\n",
    "        if key in type_id_pairs:\n",
    "            key_id = type_id_pairs[key]\n",
    "            type_obj = id_memo_dict[key_id]\n",
    "            if type_obj['isAbstract']:\n",
    "                local_partition = {}\n",
    "                if key_id in subclassing_graph.nodes:\n",
    "                    no_splits = len(list(subclassing_graph.successors(key_id)))\n",
    "                    taken = 0\n",
    "                    for indx, succ in enumerate(subclassing_graph.successors(key_id)):\n",
    "                        if indx < no_splits - 1:\n",
    "                            draw = random.randint(0, corrected_multiplicity_dict[key])\n",
    "                            taken = taken + draw\n",
    "                        else:\n",
    "                            draw = corrected_multiplicity_dict[key] - taken\n",
    "                        local_partition.update({get_name_by_id(succ): draw})\n",
    "                partitioned_multiplicity_dict.update({key: local_partition})\n",
    "            else:\n",
    "                partitioned_multiplicity_dict.update({key: corrected_multiplicity_dict[key]})\n",
    "                \n",
    "    partitioned_multiplicity_dicts.append(partitioned_multiplicity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_multiplicity_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-tactics",
   "metadata": {},
   "source": [
    "- [ ] TODO: Fix the dictionary to have keys as IDs, not name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_instance_dicts = []\n",
    "classifier_memo_dicts = []\n",
    "\n",
    "for step in range(0, no_alts_to_create):\n",
    "\n",
    "    classifier_instance_dict = {}\n",
    "    classifier_memo_dict = {}\n",
    "    \n",
    "    partitioned_multiplicity_dict = partitioned_multiplicity_dicts[step]\n",
    "\n",
    "    for mult_key in partitioned_multiplicity_dict:\n",
    "        instances_list = []\n",
    "        if isinstance(partitioned_multiplicity_dict[mult_key], dict):\n",
    "            for special_key in partitioned_multiplicity_dict[mult_key]:\n",
    "                sub_instances_list = []\n",
    "                instances_number = (partitioned_multiplicity_dict[mult_key][special_key] + 1)\n",
    "                for index in range(1, instances_number):\n",
    "                    new_instance = Instance(special_key, index)\n",
    "                    instances_list.append(new_instance)\n",
    "                    sub_instances_list.append(new_instance)\n",
    "                    classifier_memo_dict.update({new_instance.name: [special_key, mult_key]})\n",
    "                classifier_instance_dict.update({special_key: sub_instances_list})\n",
    "            classifier_instance_dict.update({mult_key: instances_list})\n",
    "        else:\n",
    "            instances_number = (corrected_multiplicity_dict[mult_key] + 1)\n",
    "            for index in range(1, instances_number):\n",
    "                new_instance = Instance(mult_key, index)\n",
    "                instances_list.append(new_instance)\n",
    "                classifier_memo_dict.update({new_instance.name: [mult_key]})\n",
    "            classifier_instance_dict.update({mult_key: instances_list})\n",
    "            \n",
    "    classifier_instance_dicts.append(classifier_instance_dict)\n",
    "    classifier_memo_dicts.append(classifier_memo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(classifier_instance_dicts[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-registrar",
   "metadata": {},
   "source": [
    "Now that the universe of instances has been calculated, we can look at how they can be sequenced. This method is probably fragile so will need revisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "[get_name_by_id(node) for node in NX.topological_sort(part_featuring_graph.reverse())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp in NX.connected_components(part_featuring_graph.to_undirected()):\n",
    "    connected_sub = NX.subgraph(part_featuring_graph, list(comp))\n",
    "    sorted_feature_groups.append(\n",
    "        [node for node in NX.topological_sort(connected_sub.reverse())]\n",
    "    )\n",
    "    print([get_name_by_id(node) for node in NX.topological_sort(connected_sub.reverse())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier_instance_dict in classifier_instance_dicts:\n",
    "\n",
    "    for part_tree_root in part_roots:\n",
    "        root_name = get_name_by_id(part_tree_root['@id'])\n",
    "        if root_name not in classifier_instance_dict:\n",
    "            classifier_instance_dict.update({root_name: Instance(root_name, 1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-scott",
   "metadata": {},
   "source": [
    "The topological sort on the graph of feature membership determines the order in which to build up sets of instances for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sequence_dictionaries = []\n",
    "\n",
    "for step in range(0, no_alts_to_create):\n",
    "    \n",
    "    classifier_instance_dict = classifier_instance_dicts[step]\n",
    "    \n",
    "    feature_sequence_dictionary = {}\n",
    "    covered_draw_dict = {}\n",
    "    \n",
    "    for sorting in sorted_feature_groups:\n",
    "        for indx, node in enumerate(sorting):\n",
    "            if indx > 0:\n",
    "                new_dict = {}\n",
    "                # get current parent from the graph\n",
    "                for pred in part_featuring_graph.successors(node):\n",
    "                    current_parent = pred\n",
    "                    #print(get_name_by_id(current_parent))\n",
    "\n",
    "                sequence_of_sequences = []\n",
    "\n",
    "                for sequence in feature_sequence_dictionary[current_parent]:\n",
    "\n",
    "                    test_mult = random.randint(\n",
    "                        feature_lower_multiplicity(id_memo_dict[node]),\n",
    "                        feature_upper_multiplicity(id_memo_dict[node])\n",
    "                    )\n",
    "\n",
    "                    for ind_j in range(0, test_mult):\n",
    "                        new_sequence = copy.deepcopy(sequence)\n",
    "                        # find the type of the current feature node\n",
    "                        node_type = get_feature_type_name(id_memo_dict[node])\n",
    "\n",
    "                        need_draw = True\n",
    "                        \n",
    "                        safety_count = 0\n",
    "\n",
    "                        while(need_draw and safety_count < 100):\n",
    "\n",
    "                            draw = random.randint(\n",
    "                                0,\n",
    "                                corrected_multiplicity_dict[node_type] - 1\n",
    "                            )\n",
    "                            #print(classifier_instance_dict[node_type][draw])\n",
    "\n",
    "                            if node_type in covered_draw_dict:\n",
    "                                if classifier_instance_dict[node_type][draw] in covered_draw_dict[node_type]:\n",
    "                                    pass\n",
    "                                    #need_draw = False\n",
    "                                else:\n",
    "                                    covered_draw_dict[node_type].append(classifier_instance_dict[node_type][draw])\n",
    "                                    need_draw = False\n",
    "                            else:\n",
    "                                covered_draw_dict.update({node_type : [classifier_instance_dict[node_type][draw]]})\n",
    "                                need_draw = False\n",
    "\n",
    "                            safety_count = safety_count + 1\n",
    "                            if safety_count == 99:\n",
    "                                print('Safety count hit when trying to place ' + \n",
    "                                      str(classifier_instance_dict[node_type][draw]) + ' under ' +\n",
    "                                     str(new_sequence))\n",
    "                                print('Covered dict is ' + str(covered_draw_dict[node_type]))\n",
    "                        \n",
    "                        new_sequence.append(classifier_instance_dict[node_type][draw])\n",
    "\n",
    "                        sequence_of_sequences.append(new_sequence)\n",
    "\n",
    "                feature_sequence_dictionary.update({node: sequence_of_sequences})\n",
    "\n",
    "            elif indx == 0:\n",
    "                if isinstance(classifier_instance_dict[get_name_by_id(node)], list):\n",
    "                    starter_list = []\n",
    "                    for item in classifier_instance_dict[get_name_by_id(node)]:\n",
    "                        starter_list.append([item])\n",
    "                    # handle case where main type has more than one instance\n",
    "                    feature_sequence_dictionary.update({node: starter_list})\n",
    "                    #print(starter_list)\n",
    "                    if len(starter_list) == 0:\n",
    "                        break\n",
    "                else:\n",
    "                    feature_sequence_dictionary.update({node: [[classifier_instance_dict[get_name_by_id(node)]]]})\n",
    "            \n",
    "    feature_sequence_dictionaries.append(feature_sequence_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx, feature_sequence_dictionary in enumerate(feature_sequence_dictionaries):\n",
    "    if indx < 3:\n",
    "        print(\"Solution #\" + str(indx))\n",
    "        for key in feature_sequence_dictionary:\n",
    "            print(get_name_by_id(key) + ', id ' + key)\n",
    "            for seq in feature_sequence_dictionary[key]:\n",
    "                print(str(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-shelter",
   "metadata": {},
   "source": [
    "One more step here is to backfill the more general collections of type with what has been developed above. To make it easier to visualize, repeat the superclassing graph from before. The only thing to add is the base library kinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_roots = [node for node in superclassing_graph.nodes if superclassing_graph.out_degree(node) == 0]\n",
    "class_roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-death",
   "metadata": {},
   "source": [
    "We want to look at the classifier_instance_dicts in order to see if there are classifiers without entries. If we find missing keys, look to their subsets to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_class_groups = []\n",
    "\n",
    "for comp in NX.connected_components(superclassing_graph.to_undirected()):\n",
    "    component_list = []\n",
    "    connected_sub = NX.subgraph(superclassing_graph, list(comp))\n",
    "    sorted_class_groups.append(\n",
    "        [node for node in NX.topological_sort(connected_sub)]\n",
    "    )\n",
    "    print([get_name_by_id(node) for node in NX.topological_sort(connected_sub)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "founds = []\n",
    "for sorted_group in sorted_class_groups:\n",
    "    for sorted_thing in sorted_group:\n",
    "        if get_name_by_id(sorted_thing) in classifier_instance_dicts[0]:\n",
    "            founds.append(sorted_thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier_instance_dict in classifier_instance_dicts:\n",
    "    for found in founds:\n",
    "        my_group = []\n",
    "        for sorted_group in sorted_class_groups:\n",
    "            for thing in sorted_group:\n",
    "                if thing == found:\n",
    "                    my_group = sorted_group\n",
    "                    break\n",
    "        for item in my_group:\n",
    "            if not item in founds:\n",
    "                try:\n",
    "                    super_path = NX.shortest_path(superclassing_graph, found, item)\n",
    "                    #print ('Found path from ' + get_name_by_id(found) + ' to ' + get_name_by_id(item))\n",
    "                    if get_name_by_id(item) not in classifier_instance_dict:\n",
    "                        classifier_instance_dict.update({get_name_by_id(item): \n",
    "                                                        copy.deepcopy(classifier_instance_dict[get_name_by_id(found)])})\n",
    "                    else:\n",
    "                        for individual in classifier_instance_dict[get_name_by_id(found)]:\n",
    "                            if individual not in classifier_instance_dict[get_name_by_id(item)]:\n",
    "                                classifier_instance_dict[get_name_by_id(item)].append(individual)\n",
    "                            #classifier_instance_dict[get_name_by_id(item)].extend(classifier_instance_dict[get_name_by_id(found)])\n",
    "                except NX.NetworkXNoPath:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "NX.draw_planar(superclassing_graph, labels=superclassing_graph_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-british",
   "metadata": {},
   "source": [
    "# Analyze and Explore Resulting Space\n",
    "\n",
    "Now that the alternatives have been generated, we can explore the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[get_name_by_id(node), node] for node in subclassing_graph.nodes if subclassing_graph.out_degree(node) == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-objective",
   "metadata": {},
   "source": [
    "## Gather AttributeUsages and literal values\n",
    "\n",
    "Find all the attribute usages so we can navigate to them from our parts library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-pound",
   "metadata": {},
   "source": [
    "TODO: Look at using FeatureValue relationships for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_literal_values(att_use):\n",
    "    literal_values = []\n",
    "    for att_member in att_use['ownedMember']:\n",
    "        if att_member['@id'] in id_memo_dict:\n",
    "            if id_memo_dict[att_member['@id']]['@type'] == 'LiteralReal':\n",
    "                literal_values.append(id_memo_dict[att_member['@id']])\n",
    "                \n",
    "    return literal_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_uses = [att_use for att_use in non_relations if att_use['@type'] == 'AttributeUsage']\n",
    "for att_use in att_uses:\n",
    "    id_memo_dict.update({att_use['@id']: att_use})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-stupid",
   "metadata": {},
   "source": [
    "Get literal reals to connect to attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "reals = [real for real in non_relations if real['@type'] == 'LiteralReal']\n",
    "for real in reals:\n",
    "    id_memo_dict.update({real['@id']: real})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-dryer",
   "metadata": {},
   "source": [
    "Create objects to hold different values per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueHolder():\n",
    "    \n",
    "    def __init__(self, path, name, value, base_att):\n",
    "        # path is list of instance references\n",
    "        self.holder_string = ''\n",
    "        for indx, step in enumerate(path):\n",
    "            if indx == 0:\n",
    "                self.holder_string = str(step)\n",
    "            else:\n",
    "                self.holder_string = self.holder_string + '.' + str(step)\n",
    "        self.holder_string = self.holder_string + '.' + name\n",
    "        self.value = value\n",
    "        self.base_att = base_att\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.value is not None:\n",
    "            return self.holder_string + ' (' + str(self.value) + ')'\n",
    "        else:\n",
    "            return self.holder_string + ' (unset)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-transcript",
   "metadata": {},
   "source": [
    "Get expressions attached to attributes as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_ele = [found for found in non_relations if found['@id'] == 'ced4f691-07ff-45dd-a2cd-687bc3bb8942']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_values = {}\n",
    "\n",
    "for att_use in att_uses:\n",
    "    if len(att_use['ownedMember']) > 0:\n",
    "        typ = id_memo_dict[att_use['owningType']['@id']]\n",
    "        for att_member in att_use['ownedMember']:\n",
    "            for value in get_att_literal_values(att_use):\n",
    "                print (att_use['name'] + ' of ' + typ['name'] + ' = ' + str(value['value']))\n",
    "                if att_use['name'] in type_values:\n",
    "                    type_values[att_use['name']].update({typ['name']: value['value']})\n",
    "                else:\n",
    "                    type_values.update({att_use['name']: {typ['name']: value['value']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-transcription",
   "metadata": {},
   "source": [
    "- [ ] TODO: Factor the above as sequences of instances rather than going back to the type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-twist",
   "metadata": {},
   "source": [
    "## Create Attribute Sequences\n",
    "\n",
    "Add attributes to the sequences for parts, and where there are values, add those to the sequence also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-statistics",
   "metadata": {},
   "source": [
    "Get the attribute use redefinition graph to see where the usage sets may have to be set equal to each other. A more general attribute that is redefined by a more specific one must have the same list of values as the specific one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "redefinitons = [redef for redef in relations if redef['@type'] == 'Redefinition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_redefinition_graph = NX.DiGraph()\n",
    "\n",
    "for redef in redefinitons:\n",
    "    try:\n",
    "        redef_name = get_data_by_id(redef['redefinedFeature']['@id'])['owningType']['@id']\n",
    "        redefining_name = get_data_by_id(redef['redefiningFeature']['@id'])['owningType']['@id']\n",
    "        attribute_redefinition_graph.add_edge(redef['redefiningFeature']['@id'],redef['redefinedFeature']['@id'])\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "redef_orders = []\n",
    "\n",
    "for comp in NX.connected_components(attribute_redefinition_graph.to_undirected()):\n",
    "    connected_sub = NX.subgraph(attribute_redefinition_graph, list(comp))\n",
    "    sorted_feature_groups.append(\n",
    "        [node for node in NX.topological_sort(connected_sub)]\n",
    "    )\n",
    "    look_upward = []\n",
    "    redef_list = []\n",
    "    for node in NX.topological_sort(connected_sub):\n",
    "        owner_name = get_data_by_id(get_data_by_id(node)['owningType']['@id'])['name']\n",
    "        redef_list.append(node)\n",
    "        look_upward.append(owner_name + '::' + get_name_by_id(node))\n",
    "    print(look_upward)\n",
    "    redef_orders.append(redef_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-taiwan",
   "metadata": {},
   "source": [
    "Get the attribute to value graph in order to track which attributes have values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_member_value_graph = NX.DiGraph()\n",
    "\n",
    "for att in att_uses:\n",
    "    if att['@id'] in id_memo_dict:\n",
    "        if id_memo_dict[att['@id']]['@type'] == 'AttributeUsage':\n",
    "            attribute_member_value_graph.add_edge(att['@id'], att['owningType']['@id'])\n",
    "            for value in get_att_literal_values(att):\n",
    "                attribute_member_value_graph.add_edge(value['@id'], att['@id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_member_value_graph_labels = {}\n",
    "for node in attribute_member_value_graph.nodes():\n",
    "    if id_memo_dict[node]['@type'] == 'AttributeUsage':\n",
    "        attribute_member_value_graph_labels.update({node: get_name_by_id(node)})\n",
    "    elif id_memo_dict[node]['@type'] == 'LiteralReal':\n",
    "        attribute_member_value_graph_labels.update({node: id_memo_dict[node]['value']})\n",
    "    \n",
    "NX.draw_spring(attribute_member_value_graph, labels=attribute_member_value_graph_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-ethiopia",
   "metadata": {},
   "source": [
    "### Match Attributes to Classifier types\n",
    "\n",
    "Where the owningType of the Attribute usage is a classifier, use it as the base of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_instance_dicts[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-editor",
   "metadata": {},
   "source": [
    "Need to order att_uses in the following order:\n",
    "- Neither redefined or redefining\n",
    "- Least to most redefined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_atts = []\n",
    "\n",
    "for att in att_uses:\n",
    "    att_found = False\n",
    "    for redef in redefinitons:\n",
    "        if att['@id'] == redef['redefinedFeature']['@id'] or att['@id'] == redef['redefiningFeature']['@id']:\n",
    "            att_found = True\n",
    "            break\n",
    "    if not att_found:\n",
    "        print(get_name_by_id(att['owningType']['@id']) + '::' + att['name'])\n",
    "        plain_atts.append(att['@id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_atts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "redef_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_atts = []\n",
    "for att in plain_atts:\n",
    "    ordered_atts.append(id_memo_dict[att])\n",
    "for red_seq in redef_orders:\n",
    "    for att in red_seq:\n",
    "        ordered_atts.append(id_memo_dict[att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_value_dicts = []\n",
    "value_holders = []\n",
    "\n",
    "for classifier_instance_dict in classifier_instance_dicts:\n",
    "\n",
    "    att_value_dict = {}\n",
    "\n",
    "    for att in ordered_atts:\n",
    "        new_sequences = []\n",
    "        att_owning_type = id_memo_dict[att['owningType']['@id']]\n",
    "        if att_owning_type['@type'] == 'PartDefinition':\n",
    "            # need to check for redefintion here - if redefined, point to values from redefining\n",
    "            if att['@id'] not in plain_atts:\n",
    "                atts_to_grab = []\n",
    "                for redef in redef_orders:\n",
    "                    if att['@id'] in redef:\n",
    "                        for red_att in redef:\n",
    "                            if red_att != att['@id']:\n",
    "                                try:\n",
    "                                    collect_path = NX.shortest_path(attribute_redefinition_graph, red_att, att['@id'])\n",
    "                                    atts_to_grab.append(red_att)\n",
    "\n",
    "                                except NX.NetworkXNoPath:\n",
    "                                    pass\n",
    "                if len(atts_to_grab) > 0:\n",
    "                    #print('Atts to grab for ' + att['@id'] + ' are ' + str(atts_to_grab))\n",
    "                    for atg in atts_to_grab:\n",
    "                        for new_seq in att_value_dict[atg]:\n",
    "                            new_sequences.append(new_seq)\n",
    "                                \n",
    "            if att['@id'] in plain_atts or len(atts_to_grab) == 0:\n",
    "                if att_owning_type['name'] in classifier_instance_dict:\n",
    "                    for instance in classifier_instance_dict[att_owning_type['name']]:\n",
    "                        new_sequence = []\n",
    "                        new_sequence.append(instance)\n",
    "\n",
    "                        #new_sequence.append(att['name'])\n",
    "\n",
    "                        for value in get_att_literal_values(att):\n",
    "                            #new_sequence.append(value['@id'])\n",
    "                            new_holder = ValueHolder([instance], att['name'], value['value'], att)\n",
    "                            value_holders.append(new_holder)\n",
    "                            new_sequence.append(new_holder)\n",
    "\n",
    "                        if len(get_att_literal_values(att)) == 0:\n",
    "                            new_holder = ValueHolder([instance], att['name'], None, att)\n",
    "                            new_sequence.append(new_holder)\n",
    "                        new_sequences.append(new_sequence)\n",
    "\n",
    "        att_value_dict.update({att['@id']: new_sequences})\n",
    "        \n",
    "    att_value_dicts.append(att_value_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-middle",
   "metadata": {},
   "source": [
    "TODO: The block below still needs to become aware of redefines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx, feature_sequence_dictionary in enumerate(feature_sequence_dictionaries):\n",
    "    \n",
    "    att_value_dict = att_value_dicts[indx]\n",
    "    \n",
    "    for att in ordered_atts:\n",
    "        new_sequences = []\n",
    "        att_owning_type = id_memo_dict[att['owningType']['@id']]\n",
    "        if att_owning_type['@type'] == 'PartUsage':\n",
    "            if att['@id'] not in plain_atts:\n",
    "                atts_to_grab = []\n",
    "                for redef in redef_orders:\n",
    "                    if att['@id'] in redef:\n",
    "                        for red_att in redef:\n",
    "                            if red_att != att['@id']:\n",
    "                                try:\n",
    "                                    collect_path = NX.shortest_path(attribute_redefinition_graph, red_att, att['@id'])\n",
    "                                    atts_to_grab.append(red_att)\n",
    "\n",
    "                                except NX.NetworkXNoPath:\n",
    "                                    pass\n",
    "                if len(atts_to_grab) > 0:\n",
    "                    #print('Atts to grab for ' + att['@id'] + ' are ' + str(atts_to_grab))\n",
    "                    for atg in atts_to_grab:\n",
    "                        for new_seq in att_value_dict[atg]:\n",
    "                            new_sequences.append(new_seq)\n",
    "                            \n",
    "            if att_owning_type['@id'] in feature_sequence_dictionary or len(atts_to_grab) == 0:\n",
    "                for instance_sequence in feature_sequence_dictionary[att_owning_type['@id']]:\n",
    "                    new_sequence = copy.deepcopy(instance_sequence)\n",
    "                    \n",
    "                    #new_sequence.append(att['name'])\n",
    "                    \n",
    "                    for value in get_att_literal_values(att):\n",
    "                        #new_sequence.append(value['@id'])\n",
    "                        new_holder = ValueHolder(instance_sequence, att['name'], value['value'], att)\n",
    "                        value_holders.append(new_holder)\n",
    "                        new_sequence.append(new_holder)\n",
    "                        \n",
    "                    if len(get_att_literal_values(att)) == 0:\n",
    "                        new_holder = ValueHolder(instance_sequence, att['name'], None, att)\n",
    "                        new_sequence.append(new_holder)\n",
    "                    new_sequences.append(new_sequence)\n",
    "\n",
    "            att_value_dict.update({att['@id']: new_sequences})\n",
    "        \n",
    "    att_value_dicts.append(att_value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx, att_value_dict in enumerate(att_value_dicts):\n",
    "    if indx < 3:\n",
    "        print(\"Solution #\" + str(indx))\n",
    "        for key in att_value_dict:\n",
    "            print(get_name_by_id(key) + ' under ' + \\\n",
    "                  str(id_memo_dict[id_memo_dict[key]['owningType']['@id']]['name']) + ', id ' + key)\n",
    "            for indx, seq in enumerate(att_value_dict[key]):\n",
    "                if indx < 5:\n",
    "                    seq_string = []\n",
    "                    for member in seq:\n",
    "                        if isinstance(member, str) and member in id_memo_dict:\n",
    "                            obj = id_memo_dict[member]\n",
    "                            # TODO: Create unique value objects for formula mapping\n",
    "                            if obj['@type'] == 'LiteralReal':\n",
    "                                seq_string.append(str(obj['value']) + ', id ' + member)\n",
    "                        else:\n",
    "                            seq_string.append(member)\n",
    "                    print(seq_string)\n",
    "                elif indx == 5:\n",
    "                    print('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-above",
   "metadata": {},
   "source": [
    "Now we need to do this for the feature sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-terror",
   "metadata": {},
   "source": [
    "## Gather Expressions and Invocations and Build Expression Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_graph = NX.DiGraph()\n",
    "expression_graph_labels = {}\n",
    "expression_graph_ascii = ''\n",
    "\n",
    "invocations = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-geneva",
   "metadata": {},
   "source": [
    "Memoize InvocationExpressions, BlockExpressions, FeatureReferenceExpressions, and ReferenceUsages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "invocations = [invocation for invocation in non_relations if invocation['@type'] == 'InvocationExpression']\n",
    "functions = [function for function in non_relations if function['@type'] == 'Function']\n",
    "op_exprs = [function for function in non_relations if function['@type'] == 'OperatorExpression']\n",
    "block_exprs = [block_expr for block_expr in non_relations if block_expr['@type'] == 'BlockExpression']\n",
    "feature_refs = [feature_ref for feature_ref in non_relations if feature_ref['@type'] == 'FeatureReferenceExpression']\n",
    "reference_uses = [ref_use for ref_use in non_relations if ref_use['@type'] == 'ReferenceUsage']\n",
    "\n",
    "para_members = [para_member for para_member in relations if para_member['@type'] == 'ParameterMembership']\n",
    "res_expr_members = [res_expr_member for res_expr_member in relations if res_expr_member['@type'] == 'ResultExpressionMembership']\n",
    "ret_para_members = [ret_para_member for ret_para_member in relations if ret_para_member['@type'] == 'ReturnParameterMembership']\n",
    "\n",
    "for invocation in invocations:\n",
    "    id_memo_dict.update({invocation['@id']: invocation})\n",
    "    \n",
    "for function in functions:\n",
    "    id_memo_dict.update({function['@id']: function})\n",
    "    \n",
    "for op_expr in op_exprs:\n",
    "    id_memo_dict.update({op_expr['@id']: op_expr})\n",
    "    \n",
    "for block_expr in block_exprs:\n",
    "    id_memo_dict.update({block_expr['@id']: block_expr})\n",
    "    \n",
    "for feature_ref in feature_refs:\n",
    "    id_memo_dict.update({feature_ref['@id']: feature_ref})\n",
    "    \n",
    "for reference_use in reference_uses:\n",
    "    id_memo_dict.update({reference_use['@id']: reference_use})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "ofms = []\n",
    "ofms.extend(para_members)\n",
    "ofms.extend(res_expr_members)\n",
    "ofms.extend(ret_para_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-bulletin",
   "metadata": {},
   "source": [
    "Get feature memberships from the expressions to their members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fm in feature_members:\n",
    "    if fm['owningType']['@id'] in id_memo_dict:\n",
    "        if id_memo_dict[fm['owningType']['@id']]['@type'] == 'InvocationExpression':\n",
    "            expression_graph.add_edge(fm['owningType']['@id'], fm['memberFeature']['@id'])\n",
    "        if id_memo_dict[fm['owningType']['@id']]['@type'] == 'Function':\n",
    "            expression_graph.add_edge(fm['owningType']['@id'], fm['memberFeature']['@id'])\n",
    "            expression_graph_labels.update({fm['owningType']['@id']: id_memo_dict[fm['owningType']['@id']]['name']})\n",
    "        if id_memo_dict[fm['owningType']['@id']]['@type'] == 'OperatorExpression':\n",
    "            expression_graph.add_edge(fm['owningType']['@id'], fm['memberFeature']['@id'])\n",
    "            expression_graph_labels.update({fm['owningType']['@id']: id_memo_dict[fm['owningType']['@id']]['operator']})\n",
    "            if fm['memberFeature']['@id'] in id_memo_dict:\n",
    "                if id_memo_dict[fm['memberFeature']['@id']]['@type'] == 'FeatureReferenceExpression':\n",
    "                    expression_graph_labels.update({fm['memberFeature']['@id']: \n",
    "                                                   id_memo_dict[id_memo_dict[fm['memberFeature']['@id']]['referent']['@id']]['name'] + ' (FeatRef)'})\n",
    "\n",
    "for ofm in ofms:\n",
    "    if ofm['owningType']['@id'] in id_memo_dict:\n",
    "        if id_memo_dict[ofm['owningType']['@id']]['@type'] == 'BlockExpression':\n",
    "            expression_graph.add_edge(ofm['owningType']['@id'], ofm['memberFeature']['@id'])\n",
    "            if ofm['memberFeature']['@id'] in id_memo_dict:\n",
    "                if id_memo_dict[ofm['memberFeature']['@id']]['@type'] == 'FeatureReferenceExpression':\n",
    "                    expression_graph_labels.update({ofm['memberFeature']['@id']: \n",
    "                                                   id_memo_dict[id_memo_dict[ofm['memberFeature']['@id']]['referent']['@id']]['name'] + ' (FeatRef)'})\n",
    "            if get_name_by_id(ofm['memberFeature']['@id']) is not None:\n",
    "                if get_metatype_by_id(ofm['memberFeature']['@id']) == 'ReferenceUsage':\n",
    "                    ref_type_name = id_memo_dict[id_memo_dict[ofm['memberFeature']['@id']]['type'][0]['@id']]['name']\n",
    "                    expression_graph_labels.update({ofm['memberFeature']['@id']: \n",
    "                                                    get_name_by_id(ofm['memberFeature']['@id']) + ':' + ref_type_name})\n",
    "                else:\n",
    "                    expression_graph_labels.update({ofm['memberFeature']['@id']: get_name_by_id(ofm['memberFeature']['@id'])})\n",
    "                    \n",
    "for ft in feature_types:\n",
    "    if ft['typedFeature']['@id'] in id_memo_dict:\n",
    "        if id_memo_dict[ft['type']['@id']]['@type'] == 'Function':\n",
    "            expression_graph.add_edge(ft['typedFeature']['@id'], ft['type']['@id'])\n",
    "            expression_graph_labels.update({ft['type']['@id']: id_memo_dict[ft['type']['@id']]['name']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-lesson",
   "metadata": {},
   "source": [
    "Gather the body and parameters of the invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "NX.draw_planar(expression_graph, labels=expression_graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([node for node in NX.topological_sort(expression_graph)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_root = [node for node in expression_graph.nodes() if expression_graph.in_degree(node) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_root[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-coalition",
   "metadata": {},
   "source": [
    "The final step is to connect this graph to the created ValueHolders and generated sets of calculations to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_index = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-temple",
   "metadata": {},
   "source": [
    "Collect attribute usages that have an expression as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_values = [fv for fv in relations if fv['@type'] == 'FeatureValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_values = [fv for fv in relations if fv['@type'] == 'FeatureValue'\n",
    "                  and fv['value']['@id'] in expression_graph.nodes()]\n",
    "\n",
    "formula_references = [fr for fr in feature_refs if fr['@id'] in expression_graph.nodes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-interval",
   "metadata": {},
   "source": [
    "This may come up empty if there are no instances of the formula-bearing attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_value_dicts[sol_index][formula_values[0]['owningRelatedElement']['@id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-start",
   "metadata": {},
   "source": [
    "Attribute owning type is needed for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_memo_dict[formula_values[0]['owningRelatedElement']['@id']]['owningType']['@id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-longer",
   "metadata": {},
   "source": [
    "Use 'member' field to account for inheritance (I think)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_context = id_memo_dict[\n",
    "    id_memo_dict[\n",
    "        formula_values[0]['owningRelatedElement']['@id']\n",
    "    ]['owningType']['@id']\n",
    "]['member']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_context_usages = [att_item['@id'] for att_item in att_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_context_usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-conference",
   "metadata": {},
   "source": [
    "## Build Expression Graphs with Instances and Value Holders at the Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-respondent",
   "metadata": {},
   "source": [
    "Conceptually, what we want to do is create copies of the formula graph for each instance of the parameter of interest and then fill in the roles with appropriate related objects that are in scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-helen",
   "metadata": {},
   "source": [
    "## Placing Expression Graph in Context\n",
    "\n",
    "We start by putting the expression graph within the attribute usage that has it as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "step0_graphs = []\n",
    "step0_graph_label_sets = []\n",
    "\n",
    "for eq_context in att_value_dicts[sol_index][formula_values[0]['owningRelatedElement']['@id']]:\n",
    "    e_labels = {}\n",
    "    this_expression_graph = NX.DiGraph()\n",
    "    this_expression_graph.add_edge(str(eq_context[-1]), expression_root[0])\n",
    "    e_labels.update({str(eq_context[-1]): str(eq_context[-1])})\n",
    "    for edge in expression_graph.edges():\n",
    "        this_expression_graph.add_edge(edge[0], edge[1])\n",
    "        if edge[0] in expression_graph_labels:\n",
    "            e_labels.update({edge[0]: expression_graph_labels[edge[0]]})\n",
    "        if edge[1] in expression_graph_labels:\n",
    "            e_labels.update({edge[1]: expression_graph_labels[edge[1]]})\n",
    "        \n",
    "    step0_graphs.append(this_expression_graph)\n",
    "    step0_graph_label_sets.append(e_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "NX.draw_planar(step0_graphs[2], labels=step0_graph_label_sets[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-vacuum",
   "metadata": {},
   "source": [
    "### Working with the collect operator\n",
    "\n",
    "The collect operator in this case is a query in M0 space, looking for all objects that have been mapped for the attribute instance in the specific context of a given part instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "collects = [op_expr for op_expr in op_exprs if op_expr['operator'] == 'collect']\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "for collect in collects:\n",
    "        \n",
    "    collect_references = []\n",
    "    collect_lengths = {}\n",
    "    \n",
    "    for fr in formula_references:\n",
    "    \n",
    "        try:\n",
    "            collect_path = NX.shortest_path(expression_graph, collect['@id'], fr['@id'])\n",
    "            collect_references.append(fr)\n",
    "            collect_lengths.update({fr['@id']: len(collect_path)})\n",
    "            \n",
    "        except NX.NetworkXNoPath:\n",
    "            pass\n",
    "        \n",
    "    collect_ordering = {k: v for k, v in sorted(collect_lengths.items(), key=lambda item: item[1])}\n",
    "        \n",
    "    #for cr in collect_ordering.keys():\n",
    "        #print(id_memo_dict[id_memo_dict[cr]['referent']['@id']]['name'] + ' order ' +\n",
    "        #     str(collect_ordering[cr]))\n",
    "    \n",
    "    for inst_seq in att_value_dicts[sol_index][formula_values[0]['owningRelatedElement']['@id']]:\n",
    "        seeked = copy.deepcopy(inst_seq[0:-1])\n",
    "        \n",
    "        last_expansion = None\n",
    "        \n",
    "        for cr in collect_ordering.keys():\n",
    "        \n",
    "            start_reference = id_memo_dict[id_memo_dict[cr]['referent']['@id']]\n",
    "            #print(start_reference[0]['name'])\n",
    "            expansion = []\n",
    "            if start_reference['@type'] != 'AttributeUsage':\n",
    "                # get list of parts\n",
    "                for seq in feature_sequence_dictionaries[sol_index][start_reference['@id']]:\n",
    "                    # hacky; need a more proper substring match\n",
    "                    if str(seeked) == str(seq[0:-1]):\n",
    "                        expansion.append(seq)\n",
    "                #print('Ran collect for ' + start_reference['name'] + ' on ' + str(seeked) + ' with result:')\n",
    "                #print(str(expansion))\n",
    "                last_expansion = expansion\n",
    "            else:\n",
    "                # pull attributes that match list of parts\n",
    "                for seq in att_value_dicts[sol_index][start_reference['@id']]:\n",
    "                    # hacky; need a more proper substring match\n",
    "                    tank_line = [seq[-1] for seq in last_expansion]\n",
    "                    #print(tank_line)\n",
    "                    for tank in tank_line:\n",
    "                        if str(tank) == str(seq[0]):\n",
    "                            expansion.append(seq)\n",
    "                print('Ran collect for ' + start_reference['name'] + ', id ' +\n",
    "                      start_reference['@id'] + ' on ' + str(seeked) + ' with result:')\n",
    "                \n",
    "                print(str(expansion))\n",
    "                \n",
    "                result_dict.update({str(inst_seq[-1]) + '::' + collect['@id']: expansion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-sender",
   "metadata": {},
   "source": [
    "### Graph Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-plant",
   "metadata": {},
   "source": [
    "TODO: Need to figure out how to prune everything below the collect node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_graphs = []\n",
    "step1_graph_label_sets = []\n",
    "\n",
    "for jndx, step0_graph in enumerate(step0_graphs):\n",
    "    this_expression_graph = NX.DiGraph()\n",
    "    e_labels = {}\n",
    "    \n",
    "    collect_node_swap = {}\n",
    "    \n",
    "    for indx, ordered_node in enumerate(NX.topological_sort(step0_graph)):\n",
    "        base_node = None\n",
    "        if indx == 0:\n",
    "            base_label = str(ordered_node)\n",
    "            \n",
    "        # in DiGraphs, only outward edges will be brought out\n",
    "        if step0_graph.out_degree(ordered_node) > 0:\n",
    "            for edg in step0_graph.edges(ordered_node):\n",
    "                this_expression_graph.add_edge(edg[0], edg[1])\n",
    "                if ordered_node in step0_graph_label_sets[jndx] and ordered_node not in collect_node_swap:\n",
    "                    e_labels.update({ordered_node: step0_graph_label_sets[jndx][ordered_node]})\n",
    "                edg_element = get_data_by_id(edg[1])\n",
    "                if edg_element['@type'] == 'OperatorExpression' and edg_element['operator'] == 'collect':\n",
    "                    print(\"Found collect\")\n",
    "                    # need to navigate down to the ID of the value of interest\n",
    "                    result_key = base_label + '::' + edg_element['@id']\n",
    "                    e_labels.update({edg[1]: result_key})\n",
    "                    collect_node_swap.update({edg[1]: result_dict[result_key]})\n",
    "                    \n",
    "    step1_graphs.append(this_expression_graph)\n",
    "    step1_graph_label_sets.append(e_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "NX.draw_planar(step1_graphs[2], labels=step1_graph_label_sets[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-durham",
   "metadata": {},
   "source": [
    "### Using Collect Results for Sum\n",
    "\n",
    "The collect results are under an expression that is typed by Sum. This will be used to add up all of the collects in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-rubber",
   "metadata": {},
   "source": [
    "## Perform Staging Mass Analysis\n",
    "\n",
    "- [ ] TODO: Strengthen this with analyses that actually do the calculation\n",
    "- [ ] TODO: Accommodate associations between tanks and engines to make delta-V and Isp calculations work properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-context",
   "metadata": {},
   "source": [
    "This is very specific to a given calculation - need to have a kernel for expanding the calculations just as with inspecting type hierarchy and multiplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx, feature_sequence_dictionary in enumerate(feature_sequence_dictionaries):\n",
    "    no_stages = len(feature_sequence_dictionary['31a4436f-07fd-48c6-b503-9d186d925c3c'])\n",
    "    print(str(no_stages))\n",
    "    for stag in range(1, no_stages + 1):\n",
    "        stage_empty_mass = 0\n",
    "        stage_full_mass = 0\n",
    "        stage_specific_impulse = 0\n",
    "        # add up the starter mass for the stage and the burnout mass\n",
    "        stage_instance = feature_sequence_dictionary['31a4436f-07fd-48c6-b503-9d186d925c3c'][stag - 1][-1]\n",
    "        print(str(stage_instance) + \": \" + str(classifier_memo_dicts[indx][stage_instance.name]))\n",
    "        if '54c2e565-ede4-4c12-a44c-2277566a2861' in feature_sequence_dictionary:\n",
    "            booster_instances = \\\n",
    "                [booster[-1] for booster in feature_sequence_dictionary['54c2e565-ede4-4c12-a44c-2277566a2861']\n",
    "                     if str(booster[0]) == str(stage_instance)]\n",
    "            print('Booster instance length = ' + str(len(booster_instances)))\n",
    "            for booster in booster_instances:\n",
    "                for clz in classifier_memo_dicts[indx][booster.name]:\n",
    "                    if clz in partitioned_multiplicity_dicts[indx]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        empty_mass = type_values['Empty Mass'][clz]\n",
    "                        full_mass = type_values['Full Mass'][clz]\n",
    "                        specific_impulse = type_values['Specific Impulse'][clz]\n",
    "\n",
    "                        stage_empty_mass = stage_empty_mass + empty_mass\n",
    "                        stage_full_mass = stage_full_mass + full_mass\n",
    "                        stage_specific_impulse = stage_specific_impulse + specific_impulse\n",
    "            if len(booster_instances) > 0:\n",
    "                stage_specific_impulse = stage_specific_impulse / len(booster_instances)\n",
    "        \n",
    "        if stage_empty_mass > 0.0:\n",
    "            booster_delta_V = 9.81 * stage_specific_impulse * math.log(stage_full_mass / stage_empty_mass)\n",
    "        \n",
    "        if indx < 3:\n",
    "            print ('Stage booster full mass is ' + str(stage_full_mass))\n",
    "            print ('Stage booster empty mass is ' + str(stage_empty_mass))\n",
    "            print ('Stage booster specific impulse is ' + str(stage_specific_impulse))\n",
    "            print ('Stage booster delta-V is ' + str(booster_delta_V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-insight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
